{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab7dca7",
   "metadata": {},
   "source": [
    "# Week 14 Data Preprocessing\n",
    "\n",
    "Data was aggregated from two sources, one with SMILES ([Kaggle](https://www.kaggle.com/datasets/ravisinghiitbhu/fda-approved-drugs-list-and-smiles/)) and the other with function ([FDA](https://drugcentral.org/download)).\n",
    "\n",
    "SMILES strings were canonicalized using RDKit, and then embedded using ChemBERTa.\n",
    "\n",
    "The first step of preprocessing is to match the molecules in the database of FDA approved structures with the data about drug-target interactions, so we have information about the molecular targets of the drugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e881f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fuzzy matching to create name map...\n",
      "Fuzzy matching complete. Found 1940 high-confidence matches.\n",
      "Number of drugs with assigned targets: 1940\n",
      "Ratio of drugs with assigned targets: 75.08%\n",
      "Results saved to data/FDA_Approved_structures_with_function_fuzzy.csv\n",
      "           Name                                             SMILES  \\\n",
      "8      Abacavir  NC1=NC2=C(N=CN2[C@@H]2C[C@H](CO)C=C2)C(NC2CC2)=N1   \n",
      "9    Abametapir                       CC1=CC=C(N=C1)C1=CC=C(C)C=N1   \n",
      "10     Abarelix  CC(C)C[C@H](NC(=O)[C@@H](CC(N)=O)NC(=O)[C@H](C...   \n",
      "11  Abemaciclib  CCN1CCN(CC2=CC=C(NC3=NC=C(F)C(=N3)C3=CC(F)=C4N...   \n",
      "12  Abiraterone  [H][C@@]12CC=C(C3=CC=CN=C3)[C@@]1(C)CC[C@@]1([...   \n",
      "\n",
      "                                               Target  \n",
      "8   Gag-Pol polyprotein;Reverse transcriptase/RNas...  \n",
      "9   C-C chemokine receptor type 5;C-C chemokine re...  \n",
      "10            Gonadotropin-releasing hormone receptor  \n",
      "11  Potassium voltage-gated channel subfamily H me...  \n",
      "12  Corticosteroid-binding globulin;Sex hormone-bi...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rapidfuzz import fuzz\n",
    "from rapidfuzz import process\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE_SMILES = 'data/FDA_Approved_structures.csv'\n",
    "INPUT_FILE_FUNCTION = 'data/drug.target.interaction.tsv'\n",
    "OUTPUT_FILE = 'data/FDA_Approved_structures_with_function_fuzzy.csv'\n",
    "SCORE_THRESHOLD = 70 # Minimum similarity score (out of 100) for a fuzzy match\n",
    "\n",
    "# --- 1. Load DataFrames ---\n",
    "try:\n",
    "    smiles = pd.read_csv(INPUT_FILE_SMILES)\n",
    "    # Using tab separation for the target file as specified earlier\n",
    "    function = pd.read_csv(INPUT_FILE_FUNCTION, sep='\\t')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure both files are in the current directory.\")\n",
    "    # Exit gracefully if files aren't found\n",
    "    raise\n",
    "\n",
    "# --- 2. FUZZY MATCHING & MAPPING CREATION ---\n",
    "print(\"Starting fuzzy matching to create name map...\")\n",
    "\n",
    "smiles_names = smiles['Name'].dropna().unique()\n",
    "function_names = function['DRUG_NAME'].dropna().unique()\n",
    "\n",
    "# List to hold the final name mapping: (Original SMILES Name, Matched Function Name)\n",
    "name_map_data = []\n",
    "\n",
    "# Iterate through every unique name in the SMILES list\n",
    "for smiles_name in smiles_names:\n",
    "    \n",
    "    # Use process.extractOne to find the single best match in the function_names list\n",
    "    # Use token_sort_ratio for robustness against word order/salts\n",
    "    match_result = process.extractOne(\n",
    "        smiles_name, \n",
    "        function_names, \n",
    "        scorer=fuzz.token_sort_ratio,\n",
    "        score_cutoff=SCORE_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # ✅ FIX: Check if the result is NOT None (i.e., a match >= 90 was found)\n",
    "    if match_result is not None:\n",
    "        best_match, score, _ = match_result\n",
    "        \n",
    "        # Store the mapping\n",
    "        name_map_data.append({\n",
    "            'Name_Original': smiles_name,\n",
    "            'DRUG_NAME': best_match,\n",
    "            'Match_Score': score\n",
    "        })\n",
    "\n",
    "# Convert the mapping list into a DataFrame\n",
    "name_map_df = pd.DataFrame(name_map_data)\n",
    "print(f\"Fuzzy matching complete. Found {len(name_map_df)} high-confidence matches.\")\n",
    "\n",
    "# --- 3. TWO-STEP MERGE AND CLEANUP ---\n",
    "\n",
    "# Step 3a: Join SMILES to the MAP on 'Name' (exact match on the original name)\n",
    "smiles_mapped = smiles.merge(\n",
    "    name_map_df[['Name_Original', 'DRUG_NAME']],\n",
    "    left_on='Name',\n",
    "    right_on='Name_Original',\n",
    "    how='left'\n",
    ").drop(columns=['Name_Original']) \n",
    "\n",
    "# Step 3b: Join the MAPPED result to the FUNCTION data on the matched DRUG_NAME (exact match)\n",
    "smiles_merged = smiles_mapped.merge(\n",
    "    function[['DRUG_NAME', 'TARGET_NAME']],\n",
    "    on='DRUG_NAME',\n",
    "    how='left'\n",
    ").drop(columns=['DRUG_NAME']) \n",
    "\n",
    "# Rename the target column\n",
    "smiles_merged = smiles_merged.rename(columns={'TARGET_NAME': 'Target'})\n",
    "\n",
    "# --- 4. Aggregate Multiple Targets and Save ---\n",
    "# Group by the unique drug identifiers (Name and SMILES)\n",
    "smiles_final = smiles_merged.groupby(['Name', 'SMILES'], as_index=False).agg(\n",
    "    {\n",
    "        # Aggregate the Target column: drop NaNs, convert to string, find unique, join by ';'\n",
    "        'Target': lambda x: ';'.join(x.dropna().astype(str).unique())\n",
    "    }\n",
    ")\n",
    "\n",
    "# Replace any resulting empty strings (from drugs that had no target match) with NaN\n",
    "smiles_final['Target'] = smiles_final['Target'].replace('', np.nan)\n",
    "\n",
    "\n",
    "# Save the final, enriched file\n",
    "smiles_final.to_csv(OUTPUT_FILE, index=False)\n",
    "# print count of non empty targets found\n",
    "non_empty_targets = smiles_final['Target'].notna().sum()\n",
    "print(f\"Number of drugs with assigned targets: {non_empty_targets}\")\n",
    "# ratio of non empty targets\n",
    "ratio_non_empty = non_empty_targets / len(smiles_final)\n",
    "print(f\"Ratio of drugs with assigned targets: {ratio_non_empty:.2%}\")\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_FILE}\")\n",
    "# print head of first 5 drugs with targets\n",
    "print(smiles_final[smiles_final['Target'].notna()].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68902e",
   "metadata": {},
   "source": [
    "Next we calculate canonical SMILES for each molecule using RDKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b18069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:31:48] Explicit valence for atom # 84 N, 4, is greater than permitted\n",
      "[16:31:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:31:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:31:48] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[16:31:48] SMILES Parse Error: check for mistakes around position 76:\n",
      "[16:31:48] C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C\n",
      "[16:31:48] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[16:31:48] SMILES Parse Error: extra open parentheses while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[16:31:48] SMILES Parse Error: check for mistakes around position 32:\n",
      "[16:31:48] C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2\n",
      "[16:31:48] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[16:31:48] SMILES Parse Error: extra open parentheses while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[16:31:48] SMILES Parse Error: check for mistakes around position 49:\n",
      "[16:31:48] )\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2\n",
      "[16:31:48] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[16:31:48] SMILES Parse Error: extra open parentheses while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[16:31:48] SMILES Parse Error: check for mistakes around position 66:\n",
      "[16:31:48] )=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(\n",
      "[16:31:48] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[16:31:48] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n"
     ]
    }
   ],
   "source": [
    "# convert smiles to canonical using rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolToSmiles\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/FDA_Approved_structures_with_function_fuzzy.csv\")\n",
    "\n",
    "def to_canonical(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            return MolToSmiles(mol, canonical=True)\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['SMILES_Canonical'] = df['SMILES'].apply(to_canonical)\n",
    "\n",
    "# save updated file with canonical smiles\n",
    "df.to_csv('data/Canonical_SMILES.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9dc96c",
   "metadata": {},
   "source": [
    "Finally, we calculate embeddings for the canonical smiles and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee99e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.1 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "W1202 16:31:51.746000 48116 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "100%|██████████| 41/41 [00:30<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "df = pd.read_csv(\"tmp/Canonical_SMILES.csv\")\n",
    "# remove rows that are NaN in SMILES_Canonical\n",
    "df = df.dropna(subset=['SMILES_Canonical'])\n",
    "\n",
    "# batch run to avoid memory issues\n",
    "batch_size = 64\n",
    "embeddings = []\n",
    "for i in tqdm(range(0, len(df), batch_size)):\n",
    "    batch_smiles = df['SMILES_Canonical'].iloc[i:i+batch_size].tolist()\n",
    "    inputs = tokenizer(batch_smiles, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "# save embeddings to pyarrow parquet file\n",
    "\n",
    "table = pa.Table.from_pandas(pd.DataFrame(embeddings))\n",
    "pq.write_table(table, 'data/Canonical_SMILES_embeddings.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
