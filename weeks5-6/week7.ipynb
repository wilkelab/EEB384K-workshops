{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d565436",
   "metadata": {},
   "source": [
    "## Week 7 workshop\n",
    "\n",
    "In this week, we'll continue to explore the GPT-style model trained on Shakespeare text.\n",
    "\n",
    "First we import the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37596b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from plotnine import *\n",
    "\n",
    "\n",
    "sys.path.append(\"scratch-llm\")\n",
    "from model.llm import LLM\n",
    "from model.tokenizer import Tokenizer\n",
    "from helpers.config import LLMConfig, get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f749e2c",
   "metadata": {},
   "source": [
    "Next we prepare the model setup, model, and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model setup (has to be consistent with the trained model we're loading)\n",
    "llm_config = LLMConfig(\n",
    "    vocab_size = 4096,\n",
    "    seq_len = 128,\n",
    "    dim_emb = 256,\n",
    "    num_layers = 4,\n",
    "    num_heads = 8,\n",
    "    emb_dropout = 0.0,\n",
    "    ffn_dim_hidden = 4 * 256,\n",
    "    ffn_bias = False\n",
    ")\n",
    "\n",
    "# the trained tokenizer\n",
    "tokenizer = Tokenizer(\"data/tinyshakespeare.model\")\n",
    "\n",
    "# the model object\n",
    "model = LLM(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    seq_len = llm_config.seq_len,\n",
    "    dim_emb = llm_config.dim_emb,\n",
    "    num_layers = llm_config.num_layers,\n",
    "    attn_num_heads = llm_config.num_heads,\n",
    "    emb_dropout = llm_config.emb_dropout,\n",
    "    ffn_hidden_dim = llm_config.ffn_dim_hidden,\n",
    "    ffn_bias = llm_config.ffn_bias\n",
    ")\n",
    "\n",
    "# the device on which we're running this (CPU vs GPU etc.)\n",
    "device = get_device()\n",
    "\n",
    "# move the model to the appropriate GPU device\n",
    "model.to(device)\n",
    "\n",
    "# load the saved model weights\n",
    "model.load_state_dict(torch.load(\n",
    "    \"data/tinyshakespeare_llm.pt\",\n",
    "    weights_only = True,\n",
    "    map_location = device\n",
    "))\n",
    "\n",
    "# put the model into evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbad122",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "In this section we define some functions that will be used in the subsequent analysis. Skip first and then return later to read this over in detail.\n",
    "\n",
    "First, code that can process the tiny Shakespeare text file and extract all dialog said by two specific speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e28291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dialogues(speaker1, speaker2):\n",
    "    # Read file\n",
    "    with open('data/tinyshakespeare.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    speaker1_texts = []\n",
    "    speaker2_texts = []\n",
    "    current_speaker = None\n",
    "    current_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip().endswith(':'):\n",
    "            # Save previous\n",
    "            if current_speaker and current_lines:\n",
    "                text = ' '.join(current_lines)\n",
    "                if current_speaker == speaker1:\n",
    "                    speaker1_texts.append(text)\n",
    "                elif current_speaker == speaker2:\n",
    "                    speaker2_texts.append(text)\n",
    "        \n",
    "            # New speaker\n",
    "            current_speaker = line.strip().rstrip(':')\n",
    "            current_lines = []\n",
    "        elif current_speaker in [speaker1, speaker2] and line.strip():\n",
    "            current_lines.append(line.strip())\n",
    "\n",
    "    print(f\"Speaker 1: {speaker1_texts}\")\n",
    "    print(f\"Speaker 2: {speaker2_texts}\")\n",
    "\n",
    "    # Encode and limit to 128 tokens\n",
    "    speaker1_encoded = [tokenizer.sp.encode(text)[:128] for text in speaker1_texts]\n",
    "    speaker2_encoded = [tokenizer.sp.encode(text)[:128] for text in speaker2_texts]\n",
    "    return speaker1_encoded, speaker2_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0a777",
   "metadata": {},
   "source": [
    "Next, code that can classify dialogues based on mean embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f12561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_dialogues(speaker1_encoded, speaker2_encoded, model, device='cpu', layer_index=-1, n_components=20):\n",
    "    \n",
    "    def get_dialogue_embedding(token_list):\n",
    "        \"\"\"Get embedding from specified transformer layer by averaging over sequence\"\"\"\n",
    "        # Pad/truncate to sequence length\n",
    "        if len(token_list) < model.seq_len:\n",
    "            tokens = token_list + [0] * (model.seq_len - len(token_list))\n",
    "        else:\n",
    "            tokens = token_list[:model.seq_len]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        x = torch.tensor([tokens], dtype=torch.int32).to(device)\n",
    "        \n",
    "        # Store layer output\n",
    "        layer_outputs = []\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            layer_outputs.append(output.detach().clone())\n",
    "        \n",
    "        # Register hook on specific layer\n",
    "        hook = model.transformer[layer_index].register_forward_hook(hook_fn)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            _ = model(x)\n",
    "        \n",
    "        # Clean up hook\n",
    "        hook.remove()\n",
    "        \n",
    "        # Average over sequence length: (1, seq_len, dim_emb) -> (dim_emb,)\n",
    "        embedding = layer_outputs[0].squeeze(0).mean(dim=0).cpu().numpy()\n",
    "        return embedding\n",
    "    \n",
    "    # Extract embeddings from transformer layer\n",
    "    print(f\"Extracting embeddings from transformer layer {layer_index}...\")\n",
    "    speaker1_embeddings = [get_dialogue_embedding(tokens) for tokens in speaker1_encoded]\n",
    "    speaker2_embeddings = [get_dialogue_embedding(tokens) for tokens in speaker2_encoded]\n",
    "    \n",
    "    # Combine all embeddings and create labels\n",
    "    X = np.array(speaker1_embeddings + speaker2_embeddings)\n",
    "    y = np.array([0] * len(speaker1_embeddings) + [1] * len(speaker2_embeddings))  # 0=speaker1, 1=speaker2\n",
    "    \n",
    "    print(f\"\\nTotal samples: {len(X)}\")\n",
    "    print(f\"Original embedding dimension: {X.shape[1]}\")\n",
    "    print(f\"Speaker 1 dialogues: {len(speaker1_embeddings)}\")\n",
    "    print(f\"Speaker 2 dialogues: {len(speaker2_embeddings)}\")\n",
    "    \n",
    "    # ========== PCA FOR DIMENSIONALITY REDUCTION ==========\n",
    "    print(f\"\\nPerforming PCA with {n_components} components for classification...\")\n",
    "    pca_full = PCA(n_components=n_components, random_state=3953)\n",
    "    X_pca = pca_full.fit_transform(X)\n",
    "    \n",
    "    print(f\"Reduced embedding dimension: {X_pca.shape[1]}\")\n",
    "    print(f\"Total variance explained by {n_components} components: {sum(pca_full.explained_variance_ratio_):.2%}\")\n",
    "    \n",
    "    # Create PCA DataFrame\n",
    "    pca_df = pl.DataFrame({\n",
    "        'x': X_pca[:, 0],\n",
    "        'y': X_pca[:, 1],\n",
    "        'character': ['Speaker 1'] * len(speaker1_embeddings) + ['Speaker 2'] * len(speaker2_embeddings)\n",
    "    })\n",
    "    \n",
    "    # Create PCA plot\n",
    "    pca_plot = (\n",
    "        ggplot(pca_df, aes(x='x', y='y', color='character', fill='character'))\n",
    "        + geom_point(size=3, alpha=0.7)\n",
    "        + scale_color_manual(values={'Speaker 1': '#3498db', 'Speaker 2': '#e74c3c'})\n",
    "        + scale_fill_manual(values={'Speaker 1': '#3498db', 'Speaker 2': '#e74c3c'})\n",
    "        + labs(\n",
    "            x=f'PC1',\n",
    "            y=f'PC2',\n",
    "            title=f'PCA of Speaker Dialogues (Layer {layer_index})',\n",
    "            color='Character',\n",
    "            fill='Character'\n",
    "        )\n",
    "        + theme_minimal()\n",
    "        + theme(\n",
    "            figure_size=(10, 8),\n",
    "            plot_title=element_text(size=14, weight='bold'),\n",
    "            legend_position='right'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"PCA plot created\")\n",
    "    \n",
    "    # ========== CLASSIFICATION ON PCA COMPONENTS ==========\n",
    "    # Split into train and test sets using PCA-reduced features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_pca, y, test_size=0.2, random_state=164984, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    print(f\"Feature dimension for classification: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Train logistic regression classifier on PCA components\n",
    "    print(f\"\\nTraining classifier on {n_components} principal components...\")\n",
    "    clf = LogisticRegression(random_state=69424, max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"CLASSIFICATION RESULTS (using PCA components)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.2%}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2%}\")\n",
    "    \n",
    "    print(\"\\nTest Set Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred, \n",
    "                                target_names=['Speaker 1', 'Speaker 2']))\n",
    "    \n",
    "    print(\"Test Set Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(f\"                      Predicted\")\n",
    "    print(f\"                 Speaker 1  Speaker 2\")\n",
    "    print(f\"Actual Speaker 1       {cm[0,0]:3d}        {cm[0,1]:3d}\")\n",
    "    print(f\"       Speaker 2       {cm[1,0]:3d}        {cm[1,1]:3d}\")\n",
    "    \n",
    "    # Create confusion matrix visualization\n",
    "    cm_df = pl.DataFrame({\n",
    "        'actual': ['Speaker 1', 'Speaker 1', 'Speaker 2', 'Speaker 2'],\n",
    "        'predicted': ['Speaker 1', 'Speaker 2', 'Speaker 1', 'Speaker 2'],\n",
    "        'count': [cm[0,0], cm[0,1], cm[1,0], cm[1,1]]\n",
    "    })\n",
    "    \n",
    "    cm_plot = (\n",
    "        ggplot(cm_df, aes(x='predicted', y='actual', fill='count'))\n",
    "        + geom_tile(color='white', size=1.5)\n",
    "        + geom_text(aes(label='count'), size=20, color='white')\n",
    "        + scale_fill_gradient(low='#3498db', high='#e74c3c')\n",
    "        + labs(\n",
    "            x='Predicted',\n",
    "            y='Actual',\n",
    "            title=f'Confusion Matrix: Speaker 1 vs Speaker 2 (Layer {layer_index}, {n_components} PCs)',\n",
    "            fill='Count'\n",
    "        )\n",
    "        + theme_minimal()\n",
    "        + theme(\n",
    "            figure_size=(8, 6),\n",
    "            plot_title=element_text(size=14, weight='bold'),\n",
    "            axis_title=element_text(size=12)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return clf, test_accuracy, pca_plot, cm_plot, pca_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb83976",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker1 = 'ROMEO'\n",
    "speaker2 = 'JULIET'\n",
    "\n",
    "# read and encode dialogues\n",
    "speaker1_encoded, speaker2_encoded = read_dialogues(speaker1, speaker2)\n",
    "\n",
    "print(f\"\\nSpeaker 1: {len(speaker1_encoded)} dialogues\")\n",
    "print(f\"Speaker 2: {len(speaker2_encoded)} dialogues\\n\")\n",
    "\n",
    "# classify dialogues from last layer and using first n principal components\n",
    "clf, test_accuracy, pca_plot, cm_plot, pca_model = classify_dialogues(\n",
    "    speaker1_encoded, \n",
    "    speaker2_encoded, \n",
    "    model, \n",
    "    device,\n",
    "    layer_index = -1,  # or use 3 to explicitly specify layer 4\n",
    "    n_components = 20  # use first n principal components for classification\n",
    ")\n",
    "\n",
    "print(\"\\n=== PCA Plot ===\")\n",
    "pca_plot.show()\n",
    "\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "cm_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfeff60",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Change the number of components in the PCA before classification\n",
    "- Use a different random seed in the training/test split\n",
    "- Try some other speakers, for example: QUEEN ELIZABETH, KING EDWARD IV, Boy, MENENIUS, LUCIO\n",
    "- Try a different method of compressing embeddings instead of mean, for example max or min. (Hint: replace `mean(dim=0)` with `max(dim=0)[0]` or `min(dim=0)[0]`, respectively. The additional index `[0]` extracts the values rather than the indices where the values are maximal or minimal.)\n",
    "- Use embeddings from different layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
