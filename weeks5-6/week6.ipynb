{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d565436",
   "metadata": {},
   "source": [
    "## Week 6 workshop\n",
    "\n",
    "In this week, we'll continue to explore the GPT-style model trained on Shakespeare text.\n",
    "\n",
    "First we import the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37596b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import polars as pl\n",
    "#from plotnine import ggplot, aes, geom_line, labs, theme_minimal\n",
    "\n",
    "sys.path.append(\"scratch-llm\")\n",
    "from model.llm import LLM\n",
    "from model.tokenizer import Tokenizer\n",
    "from helpers.config import LLMConfig, get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f749e2c",
   "metadata": {},
   "source": [
    "Next we prepare the model setup, model, and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model setup (has to be consistent with the trained model we're loading)\n",
    "llm_config = LLMConfig(\n",
    "    vocab_size = 4096,\n",
    "    seq_len = 128,\n",
    "    dim_emb = 256,\n",
    "    num_layers = 4,\n",
    "    num_heads = 8,\n",
    "    emb_dropout = 0.0,\n",
    "    ffn_dim_hidden = 4 * 256,\n",
    "    ffn_bias = False\n",
    ")\n",
    "\n",
    "# the trained tokenizer\n",
    "tokenizer = Tokenizer(\"data/tinyshakespeare.model\")\n",
    "\n",
    "# the model object\n",
    "model = LLM(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    seq_len = llm_config.seq_len,\n",
    "    dim_emb = llm_config.dim_emb,\n",
    "    num_layers = llm_config.num_layers,\n",
    "    attn_num_heads = llm_config.num_heads,\n",
    "    emb_dropout = llm_config.emb_dropout,\n",
    "    ffn_hidden_dim = llm_config.ffn_dim_hidden,\n",
    "    ffn_bias = llm_config.ffn_bias\n",
    ")\n",
    "\n",
    "# the device on which we're running this (CPU vs GPU etc.)\n",
    "device = get_device()\n",
    "\n",
    "# move the model to the appropriate GPU device\n",
    "model.to(device)\n",
    "\n",
    "# load the saved model weights\n",
    "model.load_state_dict(torch.load(\n",
    "    \"data/tinyshakespeare_llm.pt\",\n",
    "    weights_only = True,\n",
    "    map_location = device\n",
    "))\n",
    "\n",
    "# put the model into evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96e758",
   "metadata": {},
   "source": [
    "## Exploring the tokenizer\n",
    "\n",
    "Extract all tokens and print out two specific ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract list of all tokens\n",
    "tokens = [tokenizer.sp.id_to_piece(i) for i in range(llm_config.vocab_size)]\n",
    "\n",
    "print(tokenizer.sp.piece_to_id(\"▁perforce\"))\n",
    "print(tokenizer.sp.piece_to_id(\"▁basilisk\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f63701",
   "metadata": {},
   "source": [
    "Print all tokens the tokenizer knows.  The underscore (\"▁\") in front of a token indicates the beginning of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_line = 10\n",
    "for i in range(0, len(tokens), tokens_per_line):\n",
    "    line_tokens = tokens[i:i+tokens_per_line]\n",
    "    print(f\"[{i:5d}-{min(i+tokens_per_line-1, len(tokens)-1):5d}] {' | '.join(line_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d58e7c",
   "metadata": {},
   "source": [
    "## Exploring model parameters\n",
    "\n",
    "Structure of the model parameters:\n",
    "\n",
    "- **0** :: weight matrix for **token embeddings**\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "- **1** :: **RMSNorm** parameter vector\n",
    "- **2** :: Q, K, V matrices (concatenated) for **MultiHeadAttention**\n",
    "- **3** :: weight matrix for projout part of **MultiHeadAttention**\n",
    "- **4** :: **RMSNorm** parameter vector\n",
    "- **5** :: initial weight matrix for **FeedForward (SwiGLU)** part\n",
    "- **6** :: **SwiGLU** weight matrices (concatened)\n",
    "- **7** :: **SwiGLU** bias vector\n",
    "- **8** :: final weight matrix for **FeedForward (SwiGLU)** part\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-  **9-16** :: as 1-8 but for second TransformerBlock\n",
    "- **17-24** :: third TransformerBlock\n",
    "- **25-32** :: fourth TransformerBlock\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "- **33** :: **RMSNorm** parameter vector\n",
    "- **34** :: final **projection_head** bias vector\n",
    "\n",
    "**NOTE**: there is no weight matrix for the final projection head b/c it is \"weight-tied\" to the token embeddings weight matrix (0 above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all model parameters\n",
    "parList = list(model.parameters())\n",
    "\n",
    "# extract tensor shapes for each tensor\n",
    "parShapes = [list(el.shape) for el in parList]\n",
    "parShapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec831063",
   "metadata": {},
   "source": [
    "Extracting token embeddings. These are the initial embeddings going into the transformer module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract list of all tokens (just as before)\n",
    "tokens = [tokenizer.sp.id_to_piece(i) for i in range(llm_config.vocab_size)]\n",
    "\n",
    "# obtain tensor of token embeddings and convert numpy array for downstream manipulation\n",
    "embedding_data = parList[0].cpu().detach()\n",
    "print(embedding_data.shape) # (4096, 256)\n",
    "\n",
    "# _basilisk is token 4077\n",
    "print(embedding_data[4077, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f569838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame with token column plus embedding dimensions\n",
    "embeddings = pl.DataFrame({\n",
    "    'token': tokens,\n",
    "    'token_id': [i for i in range(llm_config.vocab_size)],\n",
    "    **{f'dim_{i}': embedding_data[:, i] for i in range(embedding_data.shape[1])}\n",
    "})\n",
    "\n",
    "# print token embeddings for three chosen tokens\n",
    "target_tokens = [\"▁basilisk\", \"▁perforce\", \"▁castle\"]\n",
    "embeddings.filter(pl.col('token').is_in(target_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad7b184",
   "metadata": {},
   "source": [
    "We can extract embeddings from each layer using hooks. Hooks are call-back functions that get called when a particular part of the model is executed, and so they allow us to capture input and output of those parts of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2015096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a list to store embeddings from each transformer layer\n",
    "layer_outputs = []\n",
    "\n",
    "# hook function that stores the output of a model component\n",
    "def hook_fn(module, input, output):\n",
    "    layer_outputs.append(output.detach().clone().cpu())\n",
    "\n",
    "# register hooks on each transformer block\n",
    "hooks = []\n",
    "for block in model.transformer:\n",
    "    hook = block.register_forward_hook(hook_fn)\n",
    "    hooks.append(hook) # we need to keep a list of all hooks so we can remove them at the end\n",
    "\n",
    "# run forward pass\n",
    "prompt = tokenizer.encode(\n",
    "    \"I say, basilisk, perforce, castle.\",\n",
    "    beg_of_string = True,\n",
    "    pad_seq = True,\n",
    "    seq_len = llm_config.seq_len\n",
    ")\n",
    "inputs = torch.tensor(prompt, dtype=torch.int32).unsqueeze(0)\n",
    "\n",
    "# print the input tensor\n",
    "print(f\"The prompt input:\\n{inputs}\\n\")\n",
    "\n",
    "# generate output\n",
    "# we don't actually need the output, we just do this to call the `forward()` function of the model\n",
    "out = model(inputs.to(device))\n",
    "\n",
    "# clean up hooks (so we can run the cell again in the same session and not accumulate hooks)\n",
    "for hook in hooks:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f507ac",
   "metadata": {},
   "source": [
    "The tokens for basilisk, perforce, and castle are at positions 122, 124, and 126 in this tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pos = [122, 124, 126]\n",
    "inputs[0, token_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779fe6f",
   "metadata": {},
   "source": [
    "These are the corresponding embeddings in layer 3 (the final layer). We will have to do a bit more work to look at them easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e996d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer_outputs[3][0, token_pos, :].shape)\n",
    "layer_outputs[3][0, token_pos, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create data frame of embedding data for given layer\n",
    "def make_embedding_table(layer_outputs, layer_id):\n",
    "    df = pl.DataFrame({\n",
    "        'token': [tokenizer.sp.id_to_piece(inputs[0, i].item()) for i in range(llm_config.seq_len)],\n",
    "        'token_id': inputs[0, :],\n",
    "        **{f'dim_{i}': layer_outputs[layer_id][0, :, i] for i in range(llm_config.dim_emb)}\n",
    "    })\n",
    "    return df\n",
    "\n",
    "print(\"All layer 0 embeddings:\")\n",
    "make_embedding_table(layer_outputs, 0)[token_pos]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09a151",
   "metadata": {},
   "source": [
    "Now we print out the embedding tables only for the target tokens, plus once more the initial token embeddings for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8709ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.filter(pl.col('token').is_in(target_tokens)) # input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_embedding_table(layer_outputs, 0)[token_pos] # embeddings after layer 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb72c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_embedding_table(layer_outputs, 1)[token_pos] # embeddings after layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_embedding_table(layer_outputs, 2)[token_pos] # embeddings after layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03748aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_embedding_table(layer_outputs, 3)[token_pos] # embeddings after layer 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
