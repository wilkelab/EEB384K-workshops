{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d565436",
   "metadata": {},
   "source": [
    "## Week 5 workshop\n",
    "\n",
    "In this week, we'll train a GPT-style model on text by Shakespeare and get the model to speak like Shakespeare.\n",
    "\n",
    "The model we're using is a from-scratch implementation of a model similar to Llama 2, written by [Cl√©ment Labrugere.](https://github.com/clabrugere) His original code is on GitHub here: https://github.com/clabrugere/scratch-llm. A copy of this repository as of September 2025 is included here in the `scratch-llm` folder.\n",
    "\n",
    "First we import the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37596b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import polars as pl\n",
    "from plotnine import ggplot, aes, geom_line, labs, theme_minimal\n",
    "\n",
    "sys.path.append(\"scratch-llm\")\n",
    "from model.llm import LLM\n",
    "from model.tokenizer import Tokenizer, train_tokenizer\n",
    "\n",
    "from helpers.dataset import NextTokenPredictionDataset\n",
    "from helpers.trainer import train\n",
    "from helpers.config import LLMConfig, TrainingConfig, get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f749e2c",
   "metadata": {},
   "source": [
    "Next the setup for the model and for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = LLMConfig(\n",
    "    vocab_size = 4096,\n",
    "    seq_len = 128,\n",
    "    dim_emb = 256,\n",
    "    num_layers = 4,\n",
    "    num_heads = 8,\n",
    "    emb_dropout = 0.0,\n",
    "    ffn_dim_hidden = 4 * 256,\n",
    "    ffn_bias = False\n",
    ")\n",
    "train_config = TrainingConfig(\n",
    "    retrain_tokenizer = False,\n",
    "    device = get_device(),\n",
    "    batch_size = 64,\n",
    "    learning_rate = 3e-4,\n",
    "    weight_decay = 1e-5,\n",
    "    max_epochs = 3,\n",
    "    log_frequency = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472a483",
   "metadata": {},
   "source": [
    "Before we can train the model, we need to train the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f0fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"data/tinyshakespeare.txt\"\n",
    "output_file = Path(input_file).with_suffix(\".model\")\n",
    "\n",
    "if not output_file.exists() or train_config.retrain_tokenizer:\n",
    "    train_tokenizer(input_file, llm_config.vocab_size)\n",
    "\n",
    "tokenizer = Tokenizer(str(output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dde14",
   "metadata": {},
   "source": [
    "A demonstration of the tokenizer in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec42a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Before we proceed any further, hear me speak.\"\n",
    "print(tokenizer.sp.EncodeAsPieces(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25c54f",
   "metadata": {},
   "source": [
    "## Model setup and training\n",
    "\n",
    "Generate the model instance using the configuration options previously chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae82768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLM(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    seq_len = llm_config.seq_len,\n",
    "    dim_emb = llm_config.dim_emb,\n",
    "    num_layers = llm_config.num_layers,\n",
    "    attn_num_heads = llm_config.num_heads,\n",
    "    emb_dropout = llm_config.emb_dropout,\n",
    "    ffn_hidden_dim = llm_config.ffn_dim_hidden,\n",
    "    ffn_bias = llm_config.ffn_bias\n",
    ")\n",
    "\n",
    "params_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
    "buffer_size = sum(p.nelement() * p.element_size() for p in model.buffers())\n",
    "size = (params_size + buffer_size) / 1024**2\n",
    "\n",
    "print(f\"Total model parameters: {sum(p.numel() for p in model.parameters()):,d}\")\n",
    "print(f\"Model size: {size:.3f}MB\\n\")\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e7f77f",
   "metadata": {},
   "source": [
    "Set up the training data and the corresponding data loader. When you look carefully at the training labels, you will notice they are exactly shifted one relative to the training inputs. That's because we're training next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "ds_train = NextTokenPredictionDataset(input_file, llm_config.seq_len, tokenizer)\n",
    "\n",
    "# data loader\n",
    "dl_train = DataLoader(ds_train, batch_size = train_config.batch_size, shuffle = True)\n",
    "\n",
    "# make pytorch print more numbers in the array instead of abbreviating with ...\n",
    "torch.set_printoptions(edgeitems = 5)\n",
    "\n",
    "# print inputs and labels for the first training iteration\n",
    "for inputs, labels in dl_train:\n",
    "    print(f\"Tensor shapes\\n  input: {inputs.shape}\\n output: {labels.shape}\\n\")\n",
    "    print(f\"Inputs:\\n{inputs}\\n\")\n",
    "    print(f\"Labels:\\n{labels}\\n\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd501d",
   "metadata": {},
   "source": [
    "Next we train the model (if you set `train_llm = True`). This may take a couple of hours, depending on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc731d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_llm = False\n",
    "\n",
    "if train_llm:\n",
    "    loss_history = train(\n",
    "        model,\n",
    "        dl_train,\n",
    "        train_config.device,\n",
    "        lr = train_config.learning_rate,\n",
    "        max_epochs = train_config.max_epochs,\n",
    "        weight_decay = train_config.weight_decay,\n",
    "        log_every = train_config.log_frequency\n",
    "    )\n",
    "    # save the model\n",
    "    torch.save(model.state_dict(), \"data/tinyshakespeare_llm.pt\")\n",
    "\n",
    "    # save the history from the training run\n",
    "    df = pl.DataFrame({\n",
    "        'index': range(len(loss_history['train_loss'])),\n",
    "        'train_loss': loss_history['train_loss']\n",
    "    })\n",
    "    df.write_csv('data/loss_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec0988b",
   "metadata": {},
   "source": [
    "Plot the history of the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b16211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame back from the CSV file\n",
    "loss_history = pl.read_csv('data/loss_history.csv')\n",
    "\n",
    "plot = (\n",
    "    ggplot(loss_history, aes(x = 'index', y = 'train_loss')) +\n",
    "        geom_line() +\n",
    "        labs(\n",
    "            x = 'Step',\n",
    "            y = 'Training loss'\n",
    "        ) +\n",
    "        theme_minimal()\n",
    "    )\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbad122",
   "metadata": {},
   "source": [
    "## Using the model\n",
    "\n",
    "To use the model, we don't have to retrain. We can just load the model that we saved previously and start exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90713f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the model to the appropriate GPU device\n",
    "model.to(train_config.device)\n",
    "\n",
    "# load the saved model weights\n",
    "model.load_state_dict(torch.load(\n",
    "    \"data/tinyshakespeare_llm.pt\",\n",
    "    weights_only = True\n",
    "))\n",
    "\n",
    "# put the model into evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6213274",
   "metadata": {},
   "source": [
    "Let's start with an empty prompt to generate random text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = torch.full((1, llm_config.seq_len), tokenizer.eos_id, dtype = torch.int32)\n",
    "print(f\"The prompt input:\\n{prompt}\\n\")\n",
    "prompt = prompt.to(train_config.device)\n",
    "out = model.generate(prompt, max_seq_len = 64, top_p = 1)\n",
    "print(f\"The output in token form:\\n{out}\")\n",
    "print(f\"The output decoded:\\n{tokenizer.decode(out.tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e6e2b",
   "metadata": {},
   "source": [
    "We can also generate from a starting prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.encode(\n",
    "    \"KING HENRY VI:\",\n",
    "    beg_of_string = True,\n",
    "    pad_seq = True,\n",
    "    seq_len = llm_config.seq_len\n",
    ")\n",
    "\n",
    "# convert prompt into tensor the model can work with\n",
    "inputs = torch.tensor(prompt, dtype=torch.int32).unsqueeze(0).to(train_config.device)\n",
    "\n",
    "# print the input tensor\n",
    "print(f\"The prompt input:\\n{inputs.to(torch.device(\"cpu\"))}\\n\")\n",
    "\n",
    "# generate output\n",
    "out = model.generate(inputs, max_seq_len=64, top_p=1)\n",
    "print(f\"The output in token form:\\n{out}\")\n",
    "print(f\"The output decoded:\\n{tokenizer.decode(out.tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d58e7c",
   "metadata": {},
   "source": [
    "## Exploring model parameters\n",
    "\n",
    "Structure of the model parameters:\n",
    "\n",
    "- **0** :: weight matrix for **token embeddings**\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "- **1** :: **RMSNorm** parameter vector\n",
    "- **2** :: Q, K, V matrices (concatenated) for **MultiHeadAttention**\n",
    "- **3** :: weight matrix for projout part of **MultiHeadAttention**\n",
    "- **4** :: **RMSNorm** parameter vector\n",
    "- **5** :: initial weight matrix for **FeedForward (SwiGLU)** part\n",
    "- **6** :: **SwiGLU** weight matrices (concatened)\n",
    "- **7** :: **SwiGLU** bias vector\n",
    "- **8** :: final weight matrix for **FeedForward (SwiGLU)** part\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-  **9-16** :: as 1-8 but for second TransformerBlock\n",
    "- **17-24** ::        \"       third         \"\n",
    "- **25-32** ::        \"       fourth        \"\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "- **33** :: **RMSNorm** parameter vector\n",
    "- **34** :: final **projection_head** bias vector\n",
    "\n",
    "**NOTE**: there is no weight matrix for the final projection head b/c it is \"weight-tied\" to the token embeddings weight matrix (0 above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parList = list(model.parameters())\n",
    "len(parList) ## 35\n",
    "parShapes = [list(el.shape) for el in parList]\n",
    "parShapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec831063",
   "metadata": {},
   "source": [
    "Extracting token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a3d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract list of all tokens\n",
    "tokens = [tokenizer.sp.id_to_piece(i) for i in range(llm_config.vocab_size)]\n",
    "\n",
    "print(tokenizer.sp.piece_to_id(\"‚ñÅperforce\"))\n",
    "print(tokenizer.sp.piece_to_id(\"‚ñÅbasilisk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensor to numpy array and create DataFrame\n",
    "embedding_data = parList[0].cpu().detach().numpy()\n",
    "print(embedding_data.shape)\n",
    "\n",
    "# _basilisk is token 4077\n",
    "print(embedding_data[4077, :])\n",
    "\n",
    "# Create DataFrame with token column plus embedding dimensions\n",
    "embeddings = pl.DataFrame({\n",
    "    'token': tokens,\n",
    "    **{f'dim_{i}': embedding_data[:, i] for i in range(embedding_data.shape[1])}\n",
    "})\n",
    "\n",
    "target_tokens = [\"‚ñÅbasilisk\", \"‚ñÅperforce\", \"‚ñÅcastle\"]\n",
    "embeddings.filter(pl.col('token').is_in(target_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
